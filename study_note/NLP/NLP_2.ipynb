{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP_2 Tokenization, Stemming, and Lemmatization with SpaCy Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python for NLP: Tokenization, Stemming, and Lemmatization with SpaCy Library](https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy 和 NLTK 都是進行自然語言處理相當熱門的套件，這兩者的差異在於 NLTK 提供了許多不同的演算法，讓分析者可以自行建構模型，並且透過不同的方式去解決問題，而 SpaCy 提供的是單一能夠解決問題的最佳演算方式。本文件主要會利用 SpaCy 套件來進行 NLP 中幾個核心的操作，包括 tokenization、stemming 和 lemmatization。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在安裝完畢 SpaCy 的套件後，接下來必須安裝語言模型，用以進行後續的操作，這邊以英文語言模型作為示範。安裝完畢後，就可以呼叫套件和安裝好的模型，並將模型儲存成物件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python -m spacy download en # 安裝模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en') # 把英文模型存成 sp 物件囉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語言模型的物件可以針對句子和文章自動切分成數個 tokens，所謂 token 就是句子中具有語言涵義的單一個體。舉例來說，把 Manchester United is looking to sign a forward for $90 million 餵到模型之中，就可以利用迴圈找出該句子包含的所有 tokens，以及這些 tokens 的性質（文字```text```、文法特性```pos_```、相依關係```dep_```等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester\n",
      "United\n",
      "is\n",
      "looking\n",
      "to\n",
      "sign\n",
      "a\n",
      "forward\n",
      "for\n",
      "$\n",
      "90\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "sentence = sp('Manchester United is looking to sign a forward for $90 million')\n",
    "\n",
    "for word in sentence:\n",
    "    print(word.text) # 找出所有 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester PROPN\n",
      "United PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "to PART\n",
      "sign VERB\n",
      "a DET\n",
      "forward NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "90 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "for word in sentence:\n",
    "    print(word.text,  word.pos_) # 找出所有 tokens 以及其文法特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester PROPN compound\n",
      "United PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "to PART aux\n",
      "sign VERB xcomp\n",
      "any DET advmod\n",
      "forward ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "sentence2 = sp(u\"Manchester United isn't looking to sign any forward.\")\n",
    "\n",
    "for word in sentence2:\n",
    "    print(word.text,  word.pos_, word.dep_) # 找出所有 tokens、文法特性及相依關係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了句子以外，SpaCy 物件也能夠處理文章，並且透過迴圈搭配```sents```把每個句子找出來。此外也能夠過 index 找出特定位置存在的 token 為何，並利用```is_sent_start```確認該 token 是否為句首。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Stackabuse.\n",
      "The site with the best Python Tutorials.\n",
      "What are you looking for?\n"
     ]
    }
   ],
   "source": [
    "document = sp('Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')\n",
    "for sentence in document.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[4].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如同先前所呈現的，tokenization 就是將文章或句子切分成數個含有語意的單位，也就是 token。\n",
    "\n",
    "以下再舉一個例子，對 \"They're leaving U.K. for U.S.A.\" 這句話進行 tokenization，會發現前後引號也被視為 token 找出來了，不過電腦卻正確判斷出 U.K. 和 U.S.A. 的 . 為縮寫而非句號。\n",
    "\n",
    "而對 Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com 進行 tokenization，可以發現郵件地址成功被指認出來，然而 non-vegetarian 的 - 被視為 token 獨立出來了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "They\n",
      "'re\n",
      "leaving\n",
      "U.K.\n",
      "for\n",
      "U.S.A.\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "sentence3 = sp('\"They\\'re leaving U.K. for U.S.A.\"')\n",
    "\n",
    "for word in sentence3:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "I\n",
      "am\n",
      "non\n",
      "-\n",
      "vegetarian\n",
      ",\n",
      "email\n",
      "me\n",
      "the\n",
      "menu\n",
      "at\n",
      "abc-xyz@gmai.com\n"
     ]
    }
   ],
   "source": [
    "sentence4 = sp(\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
    "\n",
    "for word in sentence4:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence4) # 對 tokenize 過後的物件取 len() 可以直接計算 tokens 的數量喔！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了找出一個句子的所有 tokens 之外，有時候也需要找出句子當中的 entities，也就是組織、地方、公司或者人名等等。之所以要特別提出 entity 的概念，是因為在一般 tokenize 的過程中，會忽略掉部分多字複合成的單一語意詞。舉例來說，針對 Manchester United is looking to sign Harry Kane for \\$90 million 進行 tokenize，Manchester United 這個詞就會被拆成兩個 tokens，Harry Kane 和 $90 million 也是。\n",
    "\n",
    "這種找出句子中特別的 entity 的過程，就是在 NLP 中所謂的 NER（Named Entity Recognition，命名實體辨別），要找出句子當中的 entities，只需要從 SpaCy 物件中找出```ents```性質即可，針對找出來的不同 entities，還能取出各自的文字```text```和類型標籤```label_```。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester\n",
      "United\n",
      "is\n",
      "looking\n",
      "to\n",
      "sign\n",
      "Harry\n",
      "Kane\n",
      "for\n",
      "$\n",
      "90\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "sentence5 = sp('Manchester United is looking to sign Harry Kane for $90 million')\n",
    "\n",
    "for word in sentence5:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United - PERSON - People, including fictional\n",
      "Harry Kane - PERSON - People, including fictional\n",
      "$90 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence5.ents: # 對 SpaCy 的 entities 進行迴圈\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_))) # 取出文字和標籤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了找出 tokens 和 entities 之外，SpaCy 套件還能夠直接找出句子當中的名詞，只要利用```noun_chunks```性質即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United\n",
      "Harry Kane\n"
     ]
    }
   ],
   "source": [
    "sentence6 = sp('Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million') \n",
    "\n",
    "for noun in sentence6.noun_chunks:\n",
    "    print(noun.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所謂 stemming 就是詞幹提取，也就是把找出一個單字的最原始型態，在英文、德文等語言當中，經常會碰到許多不同的源自同一基礎的字，例如 compute、computer、computing、computed 等等，從分析的角度來看，大多會希望這些字詞能夠被視為同一單詞。\n",
    "\n",
    "SpaCy 並沒有可以進行 stemming 的函數，因次要使用到 NLTK 套件，分別使用 Porter Stemmer 和 Snowball Stemmer。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # 產生進行 Porter Stemmer 的物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['compute', 'computer', 'computed', 'computing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token)) # 取出每個 tokens 的 root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english') # 產生進行 Snowball Stemmer 的物件\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token)) # 取出每個 tokens 的 root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據前兩個 stemming 的結果，發現都找出了 comput 作為詞幹，然而這個卻是一個不存在的單字，因此一般會使用另一個叫做 lemmatization 的方法，也就是所謂的詞形還原，把先前提到的那些同源詞，還原到一個真實出現在字典的單詞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization 的操作使用到 SpaCy 套件，對句子當中的 tokens 取出```lemma_```性質即可找出其 root form。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence7 = sp('compute computer computed computing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute compute\n",
      "computer computer\n",
      "computed compute\n",
      "computing computing\n"
     ]
    }
   ],
   "source": [
    "for word in sentence7:\n",
    "    print(word.text,  word.lemma_) # 取出 tokens 的詞和還原詞形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  ===> a\n",
      "letter  ===> letter\n",
      "has  ===> have\n",
      "been  ===> be\n",
      "written  ===> write\n",
      ",  ===> ,\n",
      "asking  ===> ask\n",
      "him  ===> -PRON-\n",
      "to  ===> to\n",
      "be  ===> be\n",
      "released  ===> release\n"
     ]
    }
   ],
   "source": [
    "sentence8 = sp(u'A letter has been written, asking him to be released')\n",
    "\n",
    "for word in sentence8:\n",
    "    print(word.text + '  ===>', word.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
