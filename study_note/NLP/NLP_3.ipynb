{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP_3 Vocabulary and Phrase Matching with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python for NLP: Vocabulary and Phrase Matching with SpaCy](https://stackabuse.com/python-for-nlp-vocabulary-and-phrase-matching-with-spacy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Matcher Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy 套件提供了```Matcher()```函式來進行詞組匹配，首先定義出想要匹配的 pattern，接著再把不同的 patterns 加到 Matcher 的物件當中，最在把目標文句放到該 Matcher 物件中進行匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # 使用 SpaCy 套件都要記得先引進語言模型\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "m_tool = Matcher(nlp.vocab) # 生成 Matcher 物件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著說明如何定義出不同的 patterns，假設想要配對到四種 patterns，分別為 quick-brown-fox、quick brown fox、quickbrownfox 和 quick brownfox。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [{'LOWER': 'quickbrownfox'}]\n",
    "p2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, {'IS_PUNCT': True}, {'LOWER': 'fox'}]\n",
    "p3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}]\n",
    "p4 =  [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* p1 對應到 quickbrownfox\n",
    "* p2 對應到 quick-brown-fox\n",
    "* p3 對應到 quick brown fox\n",
    "* p4 對應到 quick brownfox\n",
    "\n",
    "patterns 當中的 token 性質```LOWER```代表該詞組在進行匹配之前會先轉成小寫，token 性質```IS_PUNCT```代表可以是任一標點符號。\n",
    "\n",
    "所有的 patterns 都定義好了之後，就可以透過```add()```來把這些 patterns 加到先前生成的 Matcher 物件當中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tool.add('QBF', None, p1, p2, p3, p4) # QBF 代表該 Matcher 的名字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Matcher to the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來要實際針對目標文句進行匹配了，首先一樣先把目標文句放入語言模型中進行處理（tokenization），處理後的結果再放入 Matcher 物件當中。\n",
    "\n",
    "得到的結果為一個 list，裡頭包含所有被配對到的詞組，每一個詞組都透過 tuple 呈現，index 0 為該詞組所對應到的 Matcher 的 ID，這邊四個 tuple 的 index 0 都相同，是因為都是來自 QBF Matcher 匹配而生的詞組。tuple 的 index 1 代表被配對到的詞組的 starting index，index 2 則為 ending index。可以透過 for loop 清楚呈現出具體配對到了哪些字詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12825528024649263697, 1, 6), (12825528024649263697, 13, 16), (12825528024649263697, 21, 22), (12825528024649263697, 29, 31)]\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp('The quick-brown-fox jumps over the lazy dog. The quick brown fox eats well. \\\n",
    "               the quickbrownfox is dead. the dog misses the quick brownfox')\n",
    "\n",
    "phrase_matches = m_tool(sentence)\n",
    "print(phrase_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12825528024649263697 QBF 1 6 quick-brown-fox\n",
      "12825528024649263697 QBF 13 16 quick brown fox\n",
      "12825528024649263697 QBF 21 22 quickbrownfox\n",
      "12825528024649263697 QBF 29 31 quick brownfox\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id] # 找出 Matcher ID 所對應到的 Matcher 名稱\n",
    "    span = sentence[start:end] # 利用 slicing 找出匹配出的詞         \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Options for Rule-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy 套件有[正式的文件](https://spacy.io/usage/linguistic-features#adding-patterns-attributes)說明 phrase matching 還有哪些可以使用的性質。舉例來說，還可以透過```*```性質來匹配單一或更多的同一 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12825528024649263697 QBF 1 6 quick--brown--fox\n",
      "12825528024649263697 QBF 10 15 quick-brown---fox\n"
     ]
    }
   ],
   "source": [
    "m_tool.remove('QBF') # 先用 remove() 移除先前的 QBF Matcher\n",
    "\n",
    "p1 = [{'LOWER': 'quick'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'brown'}, \\\n",
    "      {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'fox'}] # 定義 pattern，加入 'OP':'*' 說明某個 token 可多次重複或不存在\n",
    "m_tool.add('QBF', None, p1)\n",
    "\n",
    "sentence = nlp('The quick--brown--fox jumps over the  quick-brown---fox')\n",
    "\n",
    "phrase_matches = m_tool(sentence)\n",
    "\n",
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase-Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了定義出 patterns 或者規則來進行詞組配對之外，另外一種更直接的方式是直接說明想要匹配的詞組，進行實作前首先爬取目標文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "scrapped_data = requests.get('https://en.wikipedia.org/wiki/Artificial_intelligence') # 取得頁面\n",
    "\n",
    "parsed_article = BeautifulSoup(scrapped_data.text, 'html.parser') # 解析網頁內容\n",
    "\n",
    "paragraphs = parsed_article.find_all('p') # 把網頁主要內文找出來\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article) # 非字母開頭的字以空字串取代\n",
    "processed_article = re.sub('\\s+', ' ', processed_article) # 任何空白字符以空字串取代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Phrase Matcher Object And Applying Matcher to the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備好了目標文件後，同樣導入語言模型並且建立 Matcher，和建立 rule-based 的```Matcher()```不同，phrase-based 使用```PhraseMatcher()```生成 Matcher 物件。接下來直接說明想要匹配的詞組，分別為 machine learning、robots 和 intelligent agents，然而在把這些匹配目標放入 Matcher 物件之前，必須放入語言模型中處理完畢，再透過```add()```放入模型中。最後找出匹配結果的流程和 rule-based Matcher 完全相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "phrases = ['machine learning', 'robots', 'intelligent agents']\n",
    "\n",
    "patterns = [nlp(text) for text in phrases] # \n",
    "\n",
    "phrase_matcher.add('AI', None, *patterns) # * 代表解壓縮符號，把 list 當中每一個 element 拆成不同的參數放入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5530044837203964789 AI 35 37 intelligent agents\n",
      "5530044837203964789 AI 259 261 machine learning\n",
      "5530044837203964789 AI 548 549 robots\n",
      "5530044837203964789 AI 1165 1167 machine learning\n",
      "5530044837203964789 AI 1480 1482 intelligent agents\n",
      "5530044837203964789 AI 3134 3136 intelligent agents\n",
      "5530044837203964789 AI 3293 3295 machine learning\n",
      "5530044837203964789 AI 3833 3834 robots\n",
      "5530044837203964789 AI 5291 5292 robots\n",
      "5530044837203964789 AI 5368 5369 robots\n",
      "5530044837203964789 AI 6850 6852 machine learning\n",
      "5530044837203964789 AI 6862 6864 machine learning\n",
      "5530044837203964789 AI 7583 7585 machine learning\n",
      "5530044837203964789 AI 7722 7724 machine learning\n",
      "5530044837203964789 AI 8089 8091 machine learning\n",
      "5530044837203964789 AI 9621 9622 robots\n",
      "5530044837203964789 AI 9686 9687 robots\n",
      "5530044837203964789 AI 10138 10140 machine learning\n",
      "5530044837203964789 AI 10470 10472 machine learning\n",
      "5530044837203964789 AI 11627 11628 robots\n",
      "5530044837203964789 AI 12104 12105 robots\n",
      "5530044837203964789 AI 13118 13119 robots\n",
      "5530044837203964789 AI 13229 13230 robots\n",
      "5530044837203964789 AI 13350 13351 robots\n",
      "5530044837203964789 AI 13394 13395 robots\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(processed_article)\n",
    "\n",
    "matched_phrases = phrase_matcher(sentence)\n",
    "\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy 套件亦提供了基本的停用字(stop words)，所謂停用字代表文句中一些經常出現卻沒有太特別意義的單詞，在進階的 NLP 處理當中經常會將這些字詞排除。語言模型的```Defaults.stop_words```可以呈現預設的停用字，```is_stop```可以檢視特定字詞是否為停用字，```Defaults.stop_words.add()```可以在套件中增加特定停用字，反之要移除則是```Defaults.stop_words.remove()```。若想要將套件中的特定字轉變為停用字，只要將其```is_stop```轉為 True 即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keep', 'six', 'therein', 'regarding', 'although', 'among', 'be', 'had', 'indeed', 'ours', 'ourselves', 'besides', 'herein', 'who', 'whoever', \"'d\", 'anyone', 'more', '‘re', 'across', 'is', 'other', 'thereupon', 'seemed', 'ca', 'fifty', 'down', 'those', 'hence', 'my', 'make', 'also', 'nothing', 'sometimes', 'third', 'whole', 'nor', 'via', 'once', 'may', 'whose', 'can', 'seems', 'hundred', 'after', 'formerly', \"'ll\", 'on', 'sometime', 'since', 'that', 'neither', 'seeming', 'under', 'beforehand', 'others', 'during', 'whence', 'back', 'wherein', 'made', '’d', 'about', 'are', 'enough', 'first', 'has', 'else', 'something', 'thence', 'she', 'one', 'eleven', 'hers', 'was', 'am', 'he', 'amount', \"n't\", '‘m', 'became', 'empty', \"'ve\", 'hereby', 'below', 'last', 'doing', 'between', 'quite', 'top', 'you', 'never', 'the', \"'s\", 'almost', 'often', 'becomes', 'ever', 'these', 'thus', 'whereupon', \"'m\", 'take', 'already', 'mostly', 'really', 'again', 'rather', 'if', 'through', 'moreover', 'which', 'show', 'so', 'yourselves', 'their', 'from', 'around', 'at', 'everyone', 'perhaps', 'get', 'her', 'go', 'have', 'only', 'ten', 'twenty', 'two', 'up', \"'re\", 'nowhere', 'an', 'a', '’ve', 'any', 'further', 'give', 'together', 'next', 'whether', 'your', '‘s', 'yourself', 'whatever', 'amongst', 'most', 'five', 'sixty', 'many', 'except', 'meanwhile', 'before', 'there', 'and', 'done', 'serious', 'anything', 'per', 'forty', 'should', 'they', 'against', 'latterly', '’s', 'but', 'himself', 'please', 're', '‘ll', 'too', 'whom', '’ll', 'than', 'every', '‘ve', 'both', 'until', 'someone', 'anyhow', 'might', 'unless', 'nobody', 'latter', 'our', 'being', 'above', 'i', 'towards', 'now', 'wherever', 'yet', 'well', 'namely', 'used', 'very', 'itself', 'no', 'name', 'over', 'everything', 'thereby', 'using', 'whereas', 'another', 'without', 'because', 'see', 'front', 'nine', 'whereby', 'toward', 'nevertheless', 'same', 'some', 'alone', 'how', 'thru', 'not', 'beside', 'hereafter', 'become', '’m', 'full', 'however', 'fifteen', 'least', 'much', 'within', 'what', 'say', 'us', 'thereafter', 'we', 'does', 'everywhere', 'afterwards', 'none', 'even', 'myself', 'such', 'side', 'therefore', 'yours', 'still', 'whenever', 'it', 'several', 'anywhere', 'whither', 'own', 'part', 'whereafter', 'bottom', 'beyond', 'seem', 'for', 'three', 'as', 'by', 'cannot', 'herself', 'just', 'somewhere', 'his', 'move', 'though', 'less', 'n‘t', 'throughout', 'eight', 'why', 'do', 'this', 'with', 'all', 'otherwise', 'would', 'onto', 'off', 'hereupon', 'while', 'could', 'will', 'must', 'always', 'few', 'me', 'becoming', 'were', '’re', 'in', 'put', 'behind', 'of', 'here', 'call', '‘d', 'did', 'upon', 'either', 'out', 'due', 'former', 'its', 'then', 'been', 'to', 'noone', 'anyway', 'mine', 'four', 'elsewhere', 'them', 'themselves', 'each', 'various', 'twelve', 'into', 'where', 'along', 'somehow', 'when', 'him', 'or', 'n’t'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "print(sp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.vocab['wonder'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.Defaults.stop_words.add('wonder')\n",
    "sp.Defaults.stop_words.remove('wonder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.vocab['wonder'].is_stop = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
